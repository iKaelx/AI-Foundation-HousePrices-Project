{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626b9ae6",
   "metadata": {},
   "source": [
    "# ðŸ“˜ **Model Comparison â€” Our Pipeline vs Their Pipeline**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŸ¦ **1. Summary of Model Results**\n",
    "\n",
    "### **ðŸ”¹ Our Results (Using Log-Transformed Price)**\n",
    "\n",
    "| Model | Train MSE | Val MSE | Test MSE | Train RÂ² | Val RÂ² | Test RÂ² |\n",
    "|-------|------------|----------|------------|-----------|---------|----------|\n",
    "| ANN | 11,045,072,268 | 16,707,050,982 | 18,859,329,406 | 0.9161 | 0.8728 | 0.8764 |\n",
    "| Random Forest | 3,053,770,088 | 14,948,286,152 | 18,118,163,837 | 0.9768 | 0.8862 | 0.8813 |\n",
    "| Decision Tree | 0.0243 | 0.0424 | 0.0460 | 0.9114 | 0.8484 | 0.8406 |\n",
    "| SVR | 0.0209 | 0.0360 | 0.0376 | 0.9237 | 0.8715 | 0.8697 |\n",
    "\n",
    "> **Note:** These scores are computed on **log(price)**, not the actual price.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ”¹ Their Results (Using Actual Price in USD)**\n",
    "\n",
    "| Model | Train RMSE | Val RMSE | Test RMSE | Test RÂ² |\n",
    "|--------|--------------|-------------|--------------|-----------|\n",
    "| Random Forest | \\$40,362 | \\$85,530 | \\$85,485 | 0.8652 |\n",
    "| Gradient Boosting | \\$68,249 | \\$86,053 | \\$87,958 | 0.8572 |\n",
    "| Decision Tree | \\$55,795 | \\$106,089 | \\$108,506 | 0.7828 |\n",
    "| Neural Network | \\$80,954 | \\$87,143 | \\$89,049 | 0.8537 |\n",
    "\n",
    "> These errors are measured directly in **dollars**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŸ¦ **2. Main Differences Between the Two Pipelines**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”µ **A. Target Variable**\n",
    "\n",
    "### **Our Pipeline**\n",
    "- Target = `log1p(price)`\n",
    "- Models learn smoother, normalized target  \n",
    "- Loss computed in log units\n",
    "\n",
    "### **Their Pipeline**\n",
    "- Target = raw `price`  \n",
    "- Metrics measured directly in \\$\n",
    "\n",
    "ðŸ‘‰ **This fundamental difference makes metrics NOT directly comparable.**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”µ **B. Feature Engineering Differences**\n",
    "\n",
    "### **Our features:**\n",
    "- Dropped: `date`, `yr_built`, `yr_renovated`, `sqft_living15`, `sqft_lot15`\n",
    "- Added:  \n",
    "  âœ” `basement_exists`  \n",
    "  âœ” imputed `sqft_basement`  \n",
    "- LabelEncoder for condition & grade  \n",
    "- Target encoding for zipcode  \n",
    "- No outlier filtering  \n",
    "\n",
    "### **Their features:**\n",
    "- Created:  \n",
    "  âœ” year & month from date  \n",
    "  âœ” house age  \n",
    "  âœ” renovation features  \n",
    "  âœ” total sqft, sqft ratios  \n",
    "  âœ” basement flag  \n",
    "- One-hot encoding for top 30 zipcodes  \n",
    "- Dropped weak features  \n",
    "- Removed outliers using z-score\n",
    "\n",
    "ðŸ‘‰ Their feature engineering is far richer and captures more variation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”µ **C. Zipcode Encoding**\n",
    "\n",
    "### **Ours â†’ Target Encoding**\n",
    "- Converts each zipcode to **1 statistical value**\n",
    "- Works well for linear/ANN\n",
    "- Can cause leakage\n",
    "\n",
    "### **Theirs â†’ One-Hot Encoding**\n",
    "- Best for tree models  \n",
    "- Preserves spatial patterns  \n",
    "\n",
    "ðŸ‘‰ This explains the strong performance of their Random Forest.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”µ **D. Data Splitting & Cleaning**\n",
    "\n",
    "### **Our Split**\n",
    "- 70/15/15 (train/val/test)\n",
    "- No outlier removal  \n",
    "\n",
    "### **Their Split**\n",
    "- 70/15/15 \n",
    "- **Removed outliers before training**  \n",
    "\n",
    "ðŸ‘‰ Cleaner data = more stable metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”µ **E. Scaling Strategy**\n",
    "\n",
    "### **Our Pipeline**\n",
    "- Scaled all numeric features  \n",
    "- Random Forest trained on scaled data (not needed)\n",
    "\n",
    "### **Their Pipeline**\n",
    "- Trees used **unscaled** data  \n",
    "- Scaling only for neural network  \n",
    "\n",
    "ðŸ‘‰ Trees are invariant to scaling â€“ their choice is optimal.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”µ **F. Evaluation Metrics**\n",
    "\n",
    "### **Our Metrics**\n",
    "- MSE in log-space  \n",
    "- RÂ² in log-space  \n",
    "\n",
    "### **Their Metrics**\n",
    "- RMSE in dollars  \n",
    "- RÂ² on real target values  \n",
    "\n",
    "ðŸ‘‰ **You cannot compare MSE(log-price) with RMSE(real-price).**\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŸ© **3. Final Summary**\n",
    "\n",
    "**The differences between the two experiments come from:**\n",
    "\n",
    "1. Different target representation (log-price vs price)  \n",
    "2. Different feature engineering  \n",
    "3. Different encoding of zipcode  \n",
    "4. Different outlier treatment  \n",
    "5. Different evaluation metrics  \n",
    "\n",
    "âž¡ **So the results are not directly comparable, and their RMSE will always appear larger because it is in real dollars.**  \n",
    "âž¡ **Our metrics measure error in logarithmic scale, not dollars.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c9064",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
