# ğŸ  Real Estate Price Prediction

**Machine Learning & Deep Learning (PyTorch ANN)**
**AI Foundation â€“ University Project**

---

## ğŸ“Œ Project Overview

This project implements a **complete, end-to-end machine learning pipeline** to predict **house prices** using the **King County House Prices dataset** (USA). The solution covers **data understanding, preprocessing, feature engineering, model training, evaluation, comparison, and saving models**.

The project compares **classical ML models** with a **deep Artificial Neural Network (ANN)** built using **PyTorch**, and analyzes **overfitting, underfitting, and generalization**.

---

## ğŸ“Š Dataset Description

**Dataset:** King County House Prices Dataset
**Samples:** 21,613 houses
**Target Variable:** `price`

### Main Features

* `bedrooms` â€“ Number of bedrooms
* `bathrooms` â€“ Number of bathrooms
* `sqft_living` â€“ Interior living area (ftÂ²)
* `sqft_lot` â€“ Land size (ftÂ²)
* `floors` â€“ Number of floors
* `waterfront` â€“ Waterfront view (0 / 1)
* `view` â€“ View quality (0â€“4)
* `condition` â€“ House condition (1â€“5)
* `grade` â€“ Construction quality (1â€“13)
* `sqft_above` â€“ Area above ground
* `sqft_basement` â€“ Basement area
* `yr_built` â€“ Construction year
* `yr_renovated` â€“ Renovation year (0 = never)
* `zipcode` â€“ Location (postal code)
* `lat`, `long` â€“ Geographic coordinates
* `sqft_living15`, `sqft_lot15` â€“ Nearby houses statistics

ğŸ“Œ **No NULL values** exist, but some columns contain **structural zeros** that behave like missing data.

---

## ğŸ” Data Quality & Analysis

### Missing-Like Values

Although the dataset has **no NULLs**, two columns need special handling:

* **`sqft_basement`**

  * `0` can mean *no basement* or *unknown size*
  * Solution:

    * Keep original column
    * Create `basement_exists` (0 / 1)
    * Create `sqft_basement_imputed` using median of non-zero values

* **`yr_renovated`**

  * `0` means *never renovated*
  * Treated as a **binary feature** (`is_renovated`)

---

## ğŸ“ˆ Outlier Detection

Outliers were analyzed using:

* **IQR (Interquartile Range)**
* **Z-Score**
* **Boxplots (single & multi-feature)**
* **Histograms + KDE**

Common outliers include:

* Houses with extremely high prices (> $5M)
* Very large houses (> 10,000 sqft)
* Abnormal bedroom counts

Outliers were **not removed aggressively**, as tree-based models and ANN can handle them effectively.

---

## ğŸ§  Problem Formulation

* **Task Type:** Regression
* **Objective:** Predict continuous house prices accurately
* **Evaluation Focus:** Generalization to unseen data

### Target Distribution

* Price distribution is **right-skewed**
* Applied **log transformation (`log1p`)** to stabilize variance and improve learning

---

## âš™ï¸ Data Preprocessing Pipeline

### 1ï¸âƒ£ Train / Validation / Test Split

* **70%** Training
* **15%** Validation
* **15%** Testing

Ensures unbiased model evaluation and proper hyperparameter tuning.

---

### 2ï¸âƒ£ Feature Engineering

* `basement_exists`
* `sqft_basement_imputed`
* `price_log` (log-transformed target)

Dropped features with low predictive value or redundancy:

* `id`
* `date`
* `yr_built`
* `yr_renovated` (replaced)
* `sqft_living15`, `sqft_lot15`

---

### 3ï¸âƒ£ Encoding Categorical Features

| Feature              | Method                   |
| -------------------- | ------------------------ |
| `zipcode`            | Target Encoding          |
| `condition`, `grade` | Label Encoding (ordinal) |
| Others               | Already numeric          |

---

### 4ï¸âƒ£ Feature Scaling

* **StandardScaler** used for:

  * Linear Regression
  * SVR
  * ANN

* **Tree-based models** (Decision Tree & Random Forest) do **not require scaling**

---

## ğŸ”¥ Models Implemented

| Model                      | Library      | Purpose                   |
| -------------------------- | ------------ | ------------------------- |
| ğŸ“ˆ Linear Regression       | scikit-learn | Baseline regression       |
| ğŸŒ² Decision Tree Regressor | scikit-learn | Interpretable model       |
| ğŸŒ³ Random Forest Regressor | scikit-learn | High-performance ensemble |
| ğŸ§  ANN                     | PyTorch      | Deep learning regression  |
| âš™ï¸ SVR                     | scikit-learn | Non-linear regression     |

---

## ğŸ§  Artificial Neural Network (ANN)

### Architecture

* Input Layer â†’ feature dimension
* Hidden Layers:

  * 256 neurons + BatchNorm + ReLU + Dropout (0.3)
  * 128 neurons + BatchNorm + ReLU + Dropout (0.2)
  * 64 neurons + ReLU
* Output Layer â†’ 1 neuron

### Training Configuration

* **Optimizer:** Adam (lr = 0.001)
* **Loss:** MSELoss
* **Scheduler:** ReduceLROnPlateau
* **Batch Size:** 32
* **Epochs:** 200

ğŸ“Œ Log-scale predictions are converted back using `expm1()`.

---

## ğŸŒ³ Random Forest Regressor

### Configuration

* `n_estimators = 301`
* `max_depth = None`
* `min_samples_split = 2`
* `min_samples_leaf = 1`
* `n_jobs = -1`

âœ” Excellent balance between **bias and variance**
âœ” Best **RÂ² score** among models

---

## ğŸŒ² Decision Tree Regressor

### Configuration

* `max_depth = 14`
* `min_samples_split = 20`
* `min_samples_leaf = 10`

âœ” Highly interpretable
âŒ Slight overfitting compared to Random Forest

---

## âš™ï¸ Support Vector Regression (SVR)

* Kernel: **RBF**
* `C = 10`
* `epsilon = 0.1`
* Strong performance on non-linear data
* Requires **feature & target scaling**

---

## ğŸ“Š Model Performance Comparison

| Model         | Train RÂ²  | Val RÂ²    | Test RÂ²   |
| ------------- | --------- | --------- | --------- |
| ANN           | ~0.91     | ~0.87     | ~0.87     |
| Random Forest | **~0.97** | **~0.89** | **~0.88** |
| Decision Tree | ~0.91     | ~0.85     | ~0.84     |
| SVR           | ~0.92     | ~0.87     | ~0.87     |

ğŸ“Œ **Best RÂ²:** Random Forest
ğŸ“Œ **Lowest MSE:** SVR

---

## âš ï¸ Overfitting & Underfitting Analysis

### Overfitting Indicators

* Large gap between train and validation scores
* Decreasing train loss while validation loss increases

### Underfitting Indicators

* Poor performance on both train & validation sets

âœ” Random Forest and SVR show **best generalization**

---

## ğŸ“‚ Project Structure

```text
ğŸ“¦ RealEstate-Price-Prediction
â”£ ğŸ“ data/ â†’ Dataset CSV
â”£ ğŸ“ models/ â†’ Saved ML & ANN models
â”£ ğŸ“„ train_models.py â†’ Full training pipeline
â”£ ğŸ“„ README.md â†’ Project documentation
â”£ ğŸ“„ requirements.txt â†’ Dependencies
```

---

## ğŸ’¾ Saved Models

* `LinearRegression.joblib`
* `DecisionTree.joblib`
* `RandomForest.joblib`
* `pytorch_ann_best.pth`
* `preprocessor.joblib`

---

## ğŸš€ How to Run

### Install dependencies

```bash
pip install -r requirements.txt
```

### Train all models

```bash
python train_models.py
```

---

## ğŸ“ Conclusion

This project demonstrates a **professional ML workflow** including:

* Data understanding & cleaning
* Feature engineering
* Model comparison
* Deep learning with PyTorch
* Performance evaluation & analysis

It highlights why **ensemble models** and **ANNs** outperform simple regressors in real-world house price prediction tasks.

---

âœ¨ *AI Foundation â€“ University Project*
